import logging
import os
import aiohttp
import asyncio
from aiogram import Bot, Dispatcher, executor, types
from pydub import AudioSegment
from typing import Optional, Dict, Any

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
BOT_TOKEN = os.getenv("BOT_TOKEN")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
ELEVEN_API_KEY = os.getenv("ELEVEN_API_KEY")
ELEVEN_VOICE_ID = "21m00Tcm4TlvDq8ikWAM"  # –ì–æ–ª–æ—Å Nova

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
if not all([BOT_TOKEN, OPENROUTER_API_KEY, ELEVEN_API_KEY]):
    raise ValueError("–ù–µ –≤—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∑–∞–¥–∞–Ω—ã!")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞
bot = Bot(token=BOT_TOKEN)
dp = Dispatcher(bot)
user_states: Dict[int, Dict[str, Any]] = {}

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
IMAGE_MODEL = "stabilityai/stable-diffusion"
TEXT_MODEL = "openchat/openchat-7b"
WHISPER_MODEL = "whisper-1"
IMAGE_SIZE = "512x512"

class AudioProcessor:
    @staticmethod
    async def convert_ogg_to_mp3(ogg_path: str, mp3_path: str) -> None:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç OGG –≤ MP3"""
        audio = AudioSegment.from_file(ogg_path)
        audio.export(mp3_path, format="mp3")

    @staticmethod
    async def cleanup_files(*files: str) -> None:
        """–£–¥–∞–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã"""
        for file in files:
            if file and os.path.exists(file):
                try:
                    os.remove(file)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {file}: {e}")

class OpenAIClient:
    def __init__(self):
        self.api_key = OPENROUTER_API_KEY
        self.api_base = "https://openrouter.ai/api/v1"

    async def transcribe_audio(self, audio_path: str) -> Optional[str]:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ –≤ —Ç–µ–∫—Å—Ç"""
        try:
            with open(audio_path, "rb") as audio_file:
                transcript = openai.Audio.transcribe(WHISPER_MODEL, audio_file)
                return transcript["text"]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏: {e}")
            return None

    async def generate_text_response(self, prompt: str) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç"""
        try:
            response = openai.ChatCompletion.create(
                model=TEXT_MODEL,
                messages=[{"role": "user", "content": prompt}]
            )
            return response['choices'][0]['message']['content']
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            return None

    async def generate_image(self, prompt: str) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ –ø—Ä–æ–º–ø—Ç—É"""
        try:
            response = openai.Image.create(
                model=IMAGE_MODEL,
                prompt=prompt,
                n=1,
                size=IMAGE_SIZE
            )
            return response['data'][0]['url']
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return None

class ElevenLabsClient:
    def __init__(self):
        self.api_key = ELEVEN_API_KEY
        self.voice_id = ELEVEN_VOICE_ID

    async def text_to_speech(self, text: str) -> Optional[bytes]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ —Ä–µ—á—å"""
        headers = {
            "xi-api-key": self.api_key,
            "Content-Type": "application/json"
        }
        payload = {
            "text": text,
            "model_id": "eleven_monolingual_v1",
            "voice_settings": {
                "stability": 0.7,
                "similarity_boost": 0.75
            }
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"https://api.elevenlabs.io/v1/text-to-speech/{self.voice_id}",
                    headers=headers,
                    json=payload
                ) as resp:
                    if resp.status == 200:
                        return await resp.read()
                    logger.error(f"–û—à–∏–±–∫–∞ ElevenLabs API: {resp.status}")
                    return None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ ElevenLabs: {e}")
            return None

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤
openai_client = OpenAIClient()
eleven_labs_client = ElevenLabsClient()

@dp.message_handler(commands=['start'])
async def start(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
    keyboard = types.ReplyKeyboardMarkup(resize_keyboard=True)
    keyboard.add("üé§ –ì–æ–≤–æ—Ä–∏", "üñº –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É")
    await message.answer("–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç ü§ñ. –ß—Ç–æ —Ö–æ—á–µ—à—å —Å–¥–µ–ª–∞—Ç—å?", reply_markup=keyboard)
    user_states[message.from_user.id] = {"waiting_for_image_prompt": False}

@dp.message_handler(lambda message: message.text and message.text != '')
async def handle_text(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    user_id = message.from_user.id
    user_state = user_states.setdefault(user_id, {"waiting_for_image_prompt": False})

    if message.text == "üé§ –ì–æ–≤–æ—Ä–∏":
        await message.reply("–ñ–¥—É –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ.")
        user_state["waiting_for_image_prompt"] = False
        return

    if message.text == "üñº –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É":
        await message.reply("–ù–∞–ø–∏—à–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ üñå")
        user_state["waiting_for_image_prompt"] = True
        return

    if user_state.get("waiting_for_image_prompt"):
        await message.reply("–ì–µ–Ω–µ—Ä–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ... ‚è≥")
        image_url = await openai_client.generate_image(message.text)
        if image_url:
            await message.reply_photo(image_url, caption="–í–æ—Ç —Ç–≤–æ—è –∫–∞—Ä—Ç–∏–Ω–∫–∞!")
        else:
            await message.reply("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ üòî")
        user_state["waiting_for_image_prompt"] = False
        return

    response = await openai_client.generate_text_response(message.text)
    if response:
        await message.reply(response)
    else:
        await message.reply("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞ üòî")

@dp.message_handler(content_types=types.ContentType.VOICE)
async def handle_voice(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    user_id = message.from_user.id
    file_prefix = f"voice_{user_id}"
    
    try:
        # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
        file_info = await bot.get_file(message.voice.file_id)
        file_url = f"https://api.telegram.org/file/bot{BOT_TOKEN}/{file_info.file_path}"
        
        ogg_file = f"{file_prefix}.ogg"
        mp3_file = f"{file_prefix}.mp3"
        tts_file = f"response_{user_id}.mp3"

        # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ
        async with aiohttp.ClientSession() as session:
            async with session.get(file_url) as resp:
                with open(ogg_file, 'wb') as f:
                    f.write(await resp.read())

        await AudioProcessor.convert_ogg_to_mp3(ogg_file, mp3_file)

        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∞—É–¥–∏–æ
        user_text = await openai_client.transcribe_audio(mp3_file)
        if not user_text:
            await message.reply("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ üòî")
            return

        await message.reply(f"–í—ã —Å–∫–∞–∑–∞–ª–∏: {user_text}")

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        reply_text = await openai_client.generate_text_response(user_text)
        if not reply_text:
            await message.reply("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç üòî")
            return

        # –û–∑–≤—É—á–∫–∞ –æ—Ç–≤–µ—Ç–∞
        audio_data = await eleven_labs_client.text_to_speech(reply_text)
        if audio_data:
            with open(tts_file, "wb") as f:
                f.write(audio_data)
            with open(tts_file, "rb") as f:
                await message.reply_voice(f, caption=reply_text)
        else:
            await message.reply(reply_text)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
        await message.reply("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è üòî")
    finally:
        # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        await AudioProcessor.cleanup_files(ogg_file, mp3_file, tts_file)

if __name__ == '__main__':
    executor.start_polling(dp, skip_updates=True)
